\section{Conclusion}

%The bug-fixing changes in APR often depend on the surrounding code
%context.
Despite their successes, the state-of-the-art DL-based APR approaches
still have limitations in the integration of code contexts in learning
bug fixes. In this work, we conjecture that correct learning of
contexts can benefit the learning of code transformations and vice
versa in auto-fixing context-dependent bugs. We introduce {\tool}, a
context-aware dual learning APR model, which dedicates one model to
learn the bug-fixing code transformations (CTL) and another one to
learn the corresponding surrounding code contexts (CCL). Instead of
cascading the two models, we train them simultaneously with
soft-sharing their parameters via a cross-stitch unit to exploit the
duality between CCL and CTL.
%We conducted several experiments to
%evaluate {\tool} on three large datasets.
Our empirical results show that {\tool} can fix 16.7\%, 12.1\%, and
14.6\% more bugs than the best-performance DL-based baseline model
with only the top-1 patches in Defects4J, Bugs.jar, and BigFix,
respectively.
%In Defects4J, it improves over the baseline models from
%16.7\%--194.7\%. In Bugs.jar and BigFix, it fixes 26.4\% and 27.7\% of
%the total fixed bugs that were missed by the best DL-based baseline.
