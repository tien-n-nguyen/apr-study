\section{Conclusion}

%The bug-fixing changes in APR often depend on the surrounding code
%context.

%Despite their successes, the state-of-the-art DL-based APR approaches
%have limitations in the integration of code contexts in learning
%context-dependent bug fixes.
We introduce {\tool}, a context-aware APR model, which
dedicates a model to learn the bug-fixing code transformations 
and another to learn code contexts. Our idea is that the mutual
improvement between the correct learning of contexts and the learning
of transformations can benefit for each other, leading to better
fixing the context-dependent bugs.
%
%Instead of cascading the two models,
We train them simultaneously
%with soft-sharing their parameters
via a cross-stitch unit.
%to exploit the duality between CCL and CTL.
%We conducted several experiments to
%evaluate {\tool} on three large datasets.
Our empirical results show that {\tool} can auto-fix
%16.7\%, 12.1\%, and 14.6\%
more bugs than the DL-based baselines with only
the top-1 patches in Bugs.jar and BigFix, respectively.
It outperforms most of state-of-the-art APR models and complement with Recoder in Defects4J.
%In Defects4J, it improves over the baseline models from
%16.7\%--194.7\%. In Bugs.jar and BigFix, it fixes 26.4\% and 27.7\% of
%the total fixed bugs that were missed by the best DL-based baseline.
