%\subsection{\bf RQ3. Impact of Dual-Task Learning on Performance}
\subsection{\bf RQ3. Impact of Context Learning and Dual-Task Learning}
\label{rq4:sec}

\definecolor{mygray}{gray}{.9}
\begin{table}[t]
  \caption{RQ3.Impact Analysis Results of Dual-Task Learning on Performance (running on BigFix Dataset).}
  \vspace{-6pt}
	{\small
	  \begin{center}
            \tabcolsep 3pt
			\renewcommand{\arraystretch}{1}
			\begin{tabular}{p{1.9cm}<{\centering}|p{1.9cm}<{\centering}|p{1.9cm}<{\centering}|p{1.9cm}<{\centering}}
				\hline
				Accuracy & Top-1 & Top-5 &  Top-10 \\			
				\hline
				DLFix   & 11.3\% & 11.8\% & 12.7\% \\ \hline
				CoCoNuT	& 12.4\% & 13.5\% & 14.1\% \\ \hline
				CURE	& 13.0\% & 13.8\% & 14.5\%\\ \hline
                                Recoder & 14.0\% & 15.2\% & 15.9\%\\ \hline
                                DEAR    & 14.1\% & 16.3\% & 17.0\%\\ \hline
			        \cellcolor{mygray} Trans-only & \cellcolor{mygray} 7.1\% & \cellcolor{mygray} 8.9\% & \cellcolor{mygray} 9.7\% \\ \hline
                                \cellcolor{mygray} Cascading & \cellcolor{mygray} 11.7\% & \cellcolor{mygray} 13.1\% & \cellcolor{mygray} 14.3\% \\ \hline
                                \cellcolor{mygray} {\tool} & \cellcolor{mygray} 14.9\% & \cellcolor{mygray} 17.1\% & \cellcolor{mygray} 18.8\% \\ \hline
				\hline
			\end{tabular}
			\label{fig:rq4_results}
		\end{center}
	}
\end{table}

%\begin{table}[t]
%  \caption{RQ3.Impact Analysis Results of Dual-Task Learning on Performance (running on BigFix Dataset).}
%  \vspace{-6pt}
%	{\small
%	  \begin{center}
%            \tabcolsep 3pt
%			\renewcommand{\arraystretch}{1}
%			\begin{tabular}{p{1cm}<{\centering}|p{3.2cm}<{\centering}|p{2cm}<{\centering}|p{1cm}<{\centering}}
%				\hline
%				Top-$K$ & \code{Transformation-only} & \code{Cascading} &  \tool \\			
%				\hline
%				Top-1   & 7.1\% & 11.7\% & 14.9\% \\ \hline
%				Top-5	& 8.9\% & 13.1\% & 16.1\% \\ \hline
%				Top-10	& 9.7\% & 14.3\% & 16.8\%\\ \hline
%			
%				\hline
%			\end{tabular}
%			\label{fig:rq4_results}
%		\end{center}
%	}
%\end{table}


%Table~\ref{fig:rq4_results} shows the comparison among {\tool} and its variants.

%{\bf 1. {\tool} and \code{Transformation-only}
%  model}.

%\subsubsection{{\bf {\tool} and \code{Transformation-only} model}}

\subsubsection{\bf Impact of Context Learning}

As seen in Table~\ref{fig:rq4_results}, \code{Trans-only} model
without context learning performs signficantly worse than all other
models that integrate context learning. Comparing with {\tool}, the
accuracy reduces: Top-$k$ values of \code{Trans-only} model are
52.3\%, 44.7\% and 42.3\% lower than those of {\tool}. This result
shows that {\em context learning from CCL} contributes positively in
{\tool}.

Comparing the result from {\tool} with the other context-aware
baseline models, we can see that {\em dedicating a separate model for
context learning in {\tool} improves APR performance}.

%In Table~\ref{fig:rq4_results}, the top-$K$ values of the
%\code{Transformation-only} model are 52.3\%, 44.7\% and 42.3\% lower
%than those of {\tool} in Top-1, Top-5, and Top-10 values,
%respectively. This result shows that {\em context learning from CCL}
%in a dual-task learning architecture contributes positively in
%{\tool}'s accuracy.

%In Table~\ref{fig:rq4_results}, the top-$K$ values of the
%\code{Transformation-only} model are 52.3\%, 44.7\% and 42.3\% lower
%than those of {\tool} in Top-1, Top-5, and Top-10 values,
%respectively. This result shows that {\em context learning}
%contributes positively in {\tool}'s accuracy, and {\em dual-task
%learning helps propagate the positive impact between CCL (context
%learning) and CTL (transformation learning)} on each other, leading
%to better fixing.


%result on the impact of our dual learning architecture on the overall
%{\tool}'s bug-fixing performance.  As seen, the top-$K$ values of the
%\code{Transformation-only} model are 52.3\%, 44.7\% and 42.3\% lower
%than those of {\tool} in Top-1, Top-5, and Top-10, respectively. This
%result shows that 1) the context learning model in {\tool} has good
%impact on the overall performance, and 2) the dual learning enables
%the impact from context learning to transformation learning to achieve
%high performance in APR.

%\vspace{3pt}
%{\bf 2. {\tool} and the \code{Cascading} model}.

%\subsubsection{{\bf {\tool} and the \code{Cascading} model}}
\subsubsection{\bf Impact of Dual-Task Learning}
\label{ccl:sec}

As seen in Table~\ref{fig:rq4_results}, the top-$K$ values of the
\code{Cascading} model are 21.5\%, 18.6\%, and 14.9\% lower than those
of {\tool} in Top-1, Top-5, and Top-10, respectively. This result
shows that cascading CCL $\rightarrow$ CTL is not as effective as dual-task
learning in {\tool}.
%
%the {\em cascading architecture between context learning (CCL) and
%  transformation learning (CTL) is not as effective as dual-task
%  learning in {\tool}}.
%
We also performed overlapping analysis between the results from
{\tool} and the \code{Cascading} model. {\tool} fixes {\bf 54} bugs
that the cascading model missed and the \code{Cascading} model fixed
only {\bf 17} bugs that {\tool} missed.
%Tien
%while both models fix the same {\bf 119} bugs.
%That is, {\tool} fixed more than 3 times unique bugs that the
%\code{Cascading} model missed than the unique bugs that were fixed by
%the \code{Cascading} model but missed by {\tool}.

To further evaluate the learning via dual-task
mechanism, we  seek the answers for the following questions.



{\em D.2.1. Does the more correct context learning in {\tool}'s
  dual-task than the context learning in 
  \code{Cascading} model lead to better bug-fixing?}

Both the \code{Cascading} model and {\tool} have the same CCL and
CTL. In the \code{Cascading} model, CCL and CTL are sequential, but
cross-stitched in {\tool}. To answer D.2.1., we first compared the
output contexts of CCL, i.e., the fixed method ASTs, from both the
\code{Cascading} model and {\tool} against the method AST in the
ground truth. If the number of common nodes between both is$>=$90\%,
we consider the output to be `correct', and `incorrect'
otherwise. Next, we considered {\em the bugs for which the output of
  CCL in the \code{Cascading} model was incorrect, and the output of
  CCL in {\tool} was correct}. Among these, the output of CTL, the
actual predicted patch, leads to more correct bug fixes for {\tool}
({\bf 134}) than for the \code{Cascading} model ({\bf 89}). This shows
that the correct context learning in {\tool}'s dual-task learning
mode has contributed to correctly fixing more bugs.

%Tien: replaced this para with the previous one
%In another study, we analyzed the set T1 of the {\bf 418} bugs in which
%the {\em outputs of the context learning model in the \code{Cascading}
%  model are incorrect} compared to the oracle. We also analyzed the
%set T2 of the {\bf 789} bugs in which {\em the outputs of the context
%  learning model in {\tool} with dual-task learning are correct}
%compared to the oracle. A correct match is defined as $\geq$ 90\%
%matching of all AST nodes in the context, otherwise, it is an
%incorrect match. Among the overlapping bugs in T1 $\cap$ T2 (i.e., the
%bugs that {\tool} correctly learns the context while the
%\code{Cascading} model did not), we reported {\bf 134} bugs that
%     {\tool} was able to fix, and {\bf 89} bugs that the
%     \code{Cascading} model fixed.  This result indicates that {\bf {\em
%       the correct context learning (thanks to {\tool}'s dual-task
%       learning) leads to more correct bug fixing}}.

{\em D.2.2. Can the better bug-fixing performance from {\tool}'s
  dual-task learning over the bug-fixing from the \code{Cascading}
  model be partially attributed to better context learning? Can the
  worse bug-fixing in the \code{Cascading} model be partially due to
  its worse context learning?}


To answer D.2.2., we analyzed {\bf 54} bugs that were fixed by
{\tool} and missed by the \code{Cascading} model. Among them, we found
{\bf 46} bugs in which {\em the output contexts of CCL in {\tool},
  i.e., the fixed method ASTs, match with the fixed method ASTs
  (contexts) in the oracle}. In contrast, we found only {\bf 18} bugs
in which the output contexts of CCL in the \code{Cascading} model
match with the fixed method ASTs.

This result indicates that better APR performance is partially due to
the better context learning in {\tool}'s dual-task mechanism than in
the cascading one.

%This indicates that {\em 1) better APR is partially due to better CCL
%  in dual-task learning} and {\em 2) a source of the inaccuracy in the
%  \code{Cascading} model is the inaccuracy of CCL} (i.e., {\bf the
%  confounding effect of the inaccuracy of CCL to those of CTL and
%  APR}).

From D.2.1 and D.2.2: Despite that both {\tool} and the
\code{cascading} model have the same CCL and CTL, {\bf {\em the better
    APR of {\tool} comes~from dual-task learning}},
    %which propagates positive impact of CCL and CTL on each other,
 making both context learning and transformation learning mutually
 better. Eventually, that leads to better APR performance in {\tool}.

%{\bf {\em the better performance of {\tool} over the \code{Cascading}
%    model comes from dual-task learning, which makes the context
%    learning more correct, leading to more correct bug-fixing}}.



%improvement in its fixing capability for the cases that the cascading
%model missed comes from the dual-task learning, which makes the
%context learning more correct, leading to more correct bug-fixing.

%In other words, dual-task learning helps the propagation of the mutual
%impact between context learning and transformation learning, leading
%to the improvement in APR.

%\vspace{3pt} {\bf 3. The \code{Cascading} model and
%  DLFix~\cite{icse20}:}
%\vspace{-2pt}


%Tien removed this
%\subsubsection{{\bf The \code{Cascading} model and DLFix~\cite{icse20}}}
%The \code{Cascading} model differs from DLFix~\cite{icse20}. First,
%CCL and CTL are different from those in
%DLFix, which uses code summarization. Second, in the \code{Cascading}
%model, the output of CCL corresponding to a buggy subtree is directly
%used as the input of CTL. In DLFix~\cite{icse20}, the summarized
%vector is used as a weight in a cross-product to represent 
%CCL $\rightarrow$ CTL.

%Although DLFix differs from the \code{Cascading} model in the CCL~and
%CTL components as well as in the ways that they connect, they~share
%the same principle of the cascading architecture. Thus, the above result
%could serve as an explanation on the reason of {\tool} improving over
%DLFix~\cite{icse20}: the dual-task learning makes the context learning
%more accurate, and as a result, more correct bug-fixing.



%Tien
%This result shows that 1) the cascading architecture between context
%learning (CCL) and transformation learning (CTL) is not effective as
%the dual-learning architecture as in {\tool}, and 2) dual learning
%between CCL and CTL is effective and helps improve APR
%performance. This result also explains the reason for the higher
%performance of {\tool} over the state-of-the-art APR approach in
%DLFix~\cite{icse20}, which has a cascading architecture of context
%learning and transformation learning.


%Table~\ref{fig:rq4_results} presents the results of contributions of dual-learning in CDFix. The results show that Only-transformation-model reduces 52.3\%, 44.7\% and 42.3\% of {\tool} using Top-1, Top-5, and Top-10, respectively, which indicates that context-learning model is important to our {\tool}.

%The Cascading model also reduces the Top-1, Top-5, and Top-10 of {\tool} by 21.5\%, 18.6\%, and 14.9\%, respectively, indicating that the simultaneous dual-learning of context learning model and transformation learning model is effective.
