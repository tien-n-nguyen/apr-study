\subsection{\bf RQ4. Impact of Dual-Task Learning on Performance}
\label{rq4:sec}


\begin{table}[t]
  \caption{RQ4.Impact Analysis Results of Dual-Task Learning on Performance (running on BigFix Dataset).}
  \vspace{-6pt}
	{\small
	  \begin{center}
            \tabcolsep 3pt
			\renewcommand{\arraystretch}{1}
			\begin{tabular}{p{1cm}<{\centering}|p{3.2cm}<{\centering}|p{2cm}<{\centering}|p{1cm}<{\centering}}
				\hline
				Top-$K$ & \code{Transformation-only} Model & \code{Cascading Model} &  \tool \\			
				\hline
				Top-1   & 7.1\% & 11.7\% & 14.9\% \\ \hline
				Top-5	& 8.9\% & 13.1\% & 16.1\% \\ \hline
				Top-10	& 9.7\% & 14.3\% & 16.8\%\\ \hline
			
				\hline
			\end{tabular}
			\label{fig:rq4_results}
		\end{center}
	}
\end{table}


%Table~\ref{fig:rq4_results} shows the comparison among {\tool} and its variants.

%{\bf 1. {\tool} and \code{Transformation-only}
%  model}.

\subsubsection{{\bf {\tool} and \code{Transformation-only}
  model}}

In Table~\ref{fig:rq4_results}, the top-$K$ values of the
\code{Transformation-only} model are 52.3\%, 44.7\% and 42.3\% lower
than those of {\tool} in Top-1, Top-5, and Top-10 values,
respectively. This result shows that {\em context learning from CCL}
in a dual-task learning architecture contributes positively in
{\tool}'s accuracy.

%In Table~\ref{fig:rq4_results}, the top-$K$ values of the
%\code{Transformation-only} model are 52.3\%, 44.7\% and 42.3\% lower
%than those of {\tool} in Top-1, Top-5, and Top-10 values,
%respectively. This result shows that {\em context learning}
%contributes positively in {\tool}'s accuracy, and {\em dual-task
%learning helps propagate the positive impact between CCL (context
%learning) and CTL (transformation learning)} on each other, leading
%to better fixing.


%result on the impact of our dual learning architecture on the overall
%{\tool}'s bug-fixing performance.  As seen, the top-$K$ values of the
%\code{Transformation-only} model are 52.3\%, 44.7\% and 42.3\% lower
%than those of {\tool} in Top-1, Top-5, and Top-10, respectively. This
%result shows that 1) the context learning model in {\tool} has good
%impact on the overall performance, and 2) the dual learning enables
%the impact from context learning to transformation learning to achieve
%high performance in APR.

%\vspace{3pt}
%{\bf 2. {\tool} and the \code{Cascading} model}.

\subsubsection{{\bf {\tool} and the \code{Cascading} model}}

As seen in Table~\ref{fig:rq4_results}, the top-$K$ values of the
\code{Cascading} model are 21.5\%, 18.6\%, and 14.9\% lower than those
of {\tool} in Top-1, Top-5, and Top-10, respectively. This result
shows that CCL $\rightarrow$ CTL is not as effective as dual-task
learning in {\tool}.
%
%the {\em cascading architecture between context learning (CCL) and
%  transformation learning (CTL) is not as effective as dual-task
%  learning in {\tool}}.
%
We also performed overlapping analysis between the results from
{\tool} and the \code{Cascading} model. {\tool} fixes {\bf 54} bugs
that the cascading model missed and the \code{Cascading} model fixed
only {\bf 17} bugs that {\tool} missed.
%Tien
%while both models fix the same {\bf 119} bugs.
%That is, {\tool} fixed more than 3 times unique bugs that the
%\code{Cascading} model missed than the unique bugs that were fixed by
%the \code{Cascading} model but missed by {\tool}.

{\em D.2.1. Do better context learning in a dual-task learning
  architecture in {\tool} lead to better APR?}

Both the \code{Cascading} model and {\tool} have CCL and CTL. In the
\code{Cascading} model, CCL and CTL are sequential, but cross-stitched
in {\tool}. To answer D.2.1, we first compared the output contexts of
CCL, i.e., the fixed method ASTs, from both the \code{Cascading} model
and {\tool} against the ground truth method AST. If the number of
common nodes between both is$>=$90\%, we consider the output to be
`correct', and `incorrect' otherwise. Next, we considered {\em the
  bugs for which the output of CCL in the \code{Cascading} model was
  incorrect, and the output of CCL in {\tool} was correct}. Among
these, the output of CTL, the actual predicted patch, leads to correct
bug-fixing in more instances ({\bf 134}) for {\tool} than for the
\code{Cascading} model ({\bf 89}). This shows correct context learning
in the CCL module in the dual-task learning architecture in {\tool}
contributed to fixing more bugs correctly.

%Tien: replaced this para with the previous one
%In another study, we analyzed the set T1 of the {\bf 418} bugs in which
%the {\em outputs of the context learning model in the \code{Cascading}
%  model are incorrect} compared to the oracle. We also analyzed the
%set T2 of the {\bf 789} bugs in which {\em the outputs of the context
%  learning model in {\tool} with dual-task learning are correct}
%compared to the oracle. A correct match is defined as $\geq$ 90\%
%matching of all AST nodes in the context, otherwise, it is an
%incorrect match. Among the overlapping bugs in T1 $\cap$ T2 (i.e., the
%bugs that {\tool} correctly learns the context while the
%\code{Cascading} model did not), we reported {\bf 134} bugs that
%     {\tool} was able to fix, and {\bf 89} bugs that the
%     \code{Cascading} model fixed.  This result indicates that {\bf {\em
%       the correct context learning (thanks to {\tool}'s dual-task
%       learning) leads to more correct bug fixing}}.

We further analyzed {\bf 54} bugs that {\tool} fixed and were missed
by the \code{Cascading} model. Among them, we found {\bf 46} bugs in
which {\em the outputs of context learning (CCL) in {\tool} match with
  the fixed contexts in the oracle}. In contrast, we found {\bf 18}
bugs in which {\em the outputs of context learning (CCL) in the
  \code{Cascading} model match with the fixed contexts}. This
indicates that {\em one source of the inaccuracy in the
  \code{Cascading} model is the inaccuracy of context learning}. This
means that {\bf {\em the better performance of {\tool} comes from
    dual-task learning, which propagates positive impact of CCL and
    CTL on each other, making both context learning and transformation
    learning better and leading to more correct bug-fixing}}.

%{\bf {\em the better performance of {\tool} over the \code{Cascading}
%    model comes from dual-task learning, which makes the context
%    learning more correct, leading to more correct bug-fixing}}.



%improvement in its fixing capability for the cases that the cascading
%model missed comes from the dual-task learning, which makes the
%context learning more correct, leading to more correct bug-fixing.

%In other words, dual-task learning helps the propagation of the mutual
%impact between context learning and transformation learning, leading
%to the improvement in APR.

%\vspace{3pt} {\bf 3. The \code{Cascading} model and
%  DLFix~\cite{icse20}:}
\vspace{-2pt}
\subsubsection{{\bf The \code{Cascading} model and DLFix~\cite{icse20}}}

The \code{Cascading} model differs from DLFix~\cite{icse20}. First,
CCL (context) and CTL (transformation) are different from those in
DLFix, which uses code summarization. Second, in the \code{Cascading}
model, the output of CCL corresponding to a buggy subtree is directly
used as the input of CTL. In DLFix~\cite{icse20}, the summarized
vector is used as a weight in a cross-product to represent the impact
from CCL to CTL.

Although DLFix differs from the \code{Cascading} model in the CCL~and
CTL components as well as in the ways that they connect, they~share
the same principle of the cascading architecture. Thus, the above result
could serve as an explanation on the reason of {\tool} improving over
DLFix~\cite{icse20}: the dual-task learning makes the context learning
more accurate, and as a result, more correct bug-fixing.



%Tien
%This result shows that 1) the cascading architecture between context
%learning (CCL) and transformation learning (CTL) is not effective as
%the dual-learning architecture as in {\tool}, and 2) dual learning
%between CCL and CTL is effective and helps improve APR
%performance. This result also explains the reason for the higher
%performance of {\tool} over the state-of-the-art APR approach in
%DLFix~\cite{icse20}, which has a cascading architecture of context
%learning and transformation learning.


%Table~\ref{fig:rq4_results} presents the results of contributions of dual-learning in CDFix. The results show that Only-transformation-model reduces 52.3\%, 44.7\% and 42.3\% of {\tool} using Top-1, Top-5, and Top-10, respectively, which indicates that context-learning model is important to our {\tool}.

%The Cascading model also reduces the Top-1, Top-5, and Top-10 of {\tool} by 21.5\%, 18.6\%, and 14.9\%, respectively, indicating that the simultaneous dual-learning of context learning model and transformation learning model is effective.
