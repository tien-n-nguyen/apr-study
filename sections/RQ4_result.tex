\subsection{\bf RQ4. Impact of Dual-Task Learning}



\begin{table}[t]
  \caption{RQ4.Impact Analysis Results of Dual Learning on Performance using BigFix Dataset.}
  \vspace{-6pt}
	{\footnotesize
		\begin{center}
			\renewcommand{\arraystretch}{1}
			\begin{tabular}{p{1cm}<{\centering}|p{2.7cm}<{\centering}|p{1.7cm}<{\centering}|p{1cm}<{\centering}}
				\hline
				Top-$K$ & Transformation-only Model & Cascading Model &  \tool \\			
				\hline
				Top-1   & 7.1\% & 11.7\% & 14.9\% \\ \hline
				Top-5	& 8.9\% & 13.1\% & 16.1\% \\ \hline
				Top-10	& 9.7\% & 14.3\% & 16.8\%\\ \hline
			
				\hline
			\end{tabular}
			\label{fig:rq4_results}
		\end{center}
	}
\end{table}

Table~\ref{fig:rq4_results} shows the comparison among {\tool} and its
variants. As seen, the top-$K$ values of the
\code{Transformation-only} model are 52.3\%, 44.7\% and 42.3\% lower
than those of {\tool} in Top-1, Top-5, and Top-10, respectively. This
shows that context learning plays an important role in {\tool} and
dual-task learning helps propagate the positive impact from CCL to
CTL.

%result on the impact of our dual learning architecture on the overall
%{\tool}'s bug-fixing performance.  As seen, the top-$K$ values of the
%\code{Transformation-only} model are 52.3\%, 44.7\% and 42.3\% lower
%than those of {\tool} in Top-1, Top-5, and Top-10, respectively. This
%result shows that 1) the context learning model in {\tool} has good
%impact on the overall performance, and 2) the dual learning enables
%the impact from context learning to transformation learning to achieve
%high performance in APR.

Moreover, the top-$K$ values of the \code{cascading} model are 21.5\%,
18.6\%, and 14.9\% lower than those of {\tool} in Top-1, Top-5, and
Top-10, respectively. This result shows that the cascading
architecture between context learning (CCL) and transformation
learning (CTL) is not as effective as the dual-task learning
in {\tool}.

We performed overlapping analysis between the results
from {\tool} and the cascading model. {\tool} fixes {\bf XX} bugs that
the cascading model missed and the cascading model fixed only {\bf XX}
bugs that {\tool} missed, while both models fix the same {\bf XXX}
bugs. This result shows that {\tool} can fix more unique bugs
that the cascading model missed than the other way.

In another study, we analyzed the set T1 of {\bf XX} bugs in which the
{\em outputs of the context learning model in the cascading model are
  incorrect} compared to the oracle. We also analyzed the set T2 of
the {\bf XX} bugs in which {\em the outputs of the context learning
  model in CDFix with dual-task learning are correct} compared to the
oracle. The correct match is defined as $\geq$ 90\% matching of all AST
nodes in the context, otherwise, it is incorrect match.
%
Among the overlapping bugs between T1 and T2, we reported {\bf XX}
bugs that {\tool} was able to fix, and {\bf XX} bugs that the
cascading model fixed.  This result indicates that the correct context
learning (caused by dual-task learning) leads to more correct bug
fixing.

We further analyzed {\bf XX} bugs that {\tool} fixed and
were missed by the cascading model. Among them, we found {\bf XX} bugs
in which {\em the outputs of CCL in the cascading model match with the
  fixed contexts in the oracle}. In contrast, we found {\bf XX} bugs
in which {\em the outputs of CCL in {\tool}
  match with the fixed contexts}. This indicates that a source
of the inaccuracy in the cascading model is the inaccuracy of context
learning. This also shows that the improvement of {\tool} comes from
dual-task learning, which makes the context learning more correct,
leading to more correct bug-fixing.

%improvement in its fixing capability for the cases that the cascading
%model missed comes from the dual-task learning, which makes the
%context learning more correct, leading to more correct bug-fixing.

%In other words, dual-task learning helps the propagation of the mutual
%impact between context learning and transformation learning, leading
%to the improvement in APR.



Although DLFix differs from the cascading model in the CCL and CTL
components as well as in the ways that they connect, they share the
same principle of cascading architecture. Thus, the above result could
serve as a good explanation on the reason {\tool} improves over
DLFix~\cite{icse20}: the dual-task learning makes the context learning
more accurate, and as a result, more correct bug-fixing.

%the correctness of the context learning model.



%Tien
%This result shows that 1) the cascading architecture between context
%learning (CCL) and transformation learning (CTL) is not effective as
%the dual-learning architecture as in {\tool}, and 2) dual learning
%between CCL and CTL is effective and helps improve APR
%performance. This result also explains the reason for the higher
%performance of {\tool} over the state-of-the-art APR approach in
%DLFix~\cite{icse20}, which has a cascading architecture of context
%learning and transformation learning.


%Table~\ref{fig:rq4_results} presents the results of contributions of dual-learning in CDFix. The results show that Only-transformation-model reduces 52.3\%, 44.7\% and 42.3\% of {\tool} using Top-1, Top-5, and Top-10, respectively, which indicates that context-learning model is important to our {\tool}.

%The Cascading model also reduces the Top-1, Top-5, and Top-10 of {\tool} by 21.5\%, 18.6\%, and 14.9\%, respectively, indicating that the simultaneous dual-learning of context learning model and transformation learning model is effective.
