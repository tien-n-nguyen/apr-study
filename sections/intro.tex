\section{Introduction}

Detecting and fixing software defects is crucial for a software
development process. To reduce the efforts from developers in that
process, several {\em fault localization} (FL)
approaches~\cite{fl-survey} have been introduced to help localize the
source of the fault that needs to be fixed. The input of an FL model
is the execution of a test suite, in which some of the test cases are
passing or failing ones. Specifically, the key input is the {\em code
  coverage matrix} in which the rows and columns correspond to the
statements and test cases, respectively.  Each cell is assigned with
the value of 1 if the respective statement is executed in the
respective test case, and with the value of 0, otherwise.  An FL model
uses such information to identify the list of {\em suspicious lines of
  code} that are ranked based on their associated {\em suspiciousness
  scores}~\cite{fl-survey}. In recent advanced FL, several approaches
also support fault localization at method
level~\cite{DeepFL,icse21-fl}. 



%In the FL problem, given the execution of test cases, an FL tool
%identifies the set of {\em suspicious lines of code} with their
%associated suspiciousness scores~\cite{fl-survey}.  The key input of
%an FL tool is the {\em code coverage matrix} in which the rows and
%columns correspond to the source code statements and test cases,
%respectively.  Each cell is assigned with the value of 1 if the
%respective statement is executed in the respective test case, and with
%the value of 0, otherwise. In recent FL, several researchers also
%advocate for fault localization at method level~\cite{DeepFL}. FL at
%both levels are useful for developers.

The FL approaches can be broadly divided into the following
categories: {\em spectrum-based fault localization} (SBFL)
approaches~\cite{Ochiai,jones2001visualization,keller2017critical},
{\em mutation-based fault localization} (MBFL)
approaches~\cite{MUSE,papadakis2012using,Metallaxis}, and {\em machine
  learning (ML)} and {\em deep learning (DL)}~\cite{DeepFL,icse21-fl}.
For SBFL approaches, the key idea is that a line covered more in the
failing test cases than in the passing ones is more suspicious than a
line executed more in the passing ones.
%
To improve SBFL, MBFL
approaches~\cite{MUSE,papadakis2012using,Metallaxis} enhance the code
coverage information by modifying a statement with mutation operators,
and then collecting code coverages when executing the mutated programs
with the test cases. The MBFL approaches apply suspiciousness score
formulas in the same manner as in SBFL approaches on the matrix for
each original statement and its mutated ones.
%
ML and DL-based FL approaches explore the code coverage matrix and
apply different neural network models for fault localization.


%{\em Spectrum-based fault localization} (SBFL)
%approaches~\cite{Ochiai,jones2001visualization,keller2017critical}
%take the recorded lines of code that were covered by each of the given
%test cases, and assigned each line of code a suspiciousness score
%based on the code coverage matrix. Despite using different
%formulas to compute that score, the idea is that a line covered more
%in the failing test cases than in the passing ones is more suspicious
%than a line executed more in the passing ones. A key drawback of those
%approaches is that the same score is given to the lines that have been
%executed in both failing and passing test cases. An example is the
%statements that are part of a block statement and executed at the
%same nested level. Another example is the conditions of the
%condition statements, e.g., \code{if}, \code{while}, \code{do},
%and \code{switch}. 

%To improve SBFL, {\em mutation-based fault localization} (MBFL)
%approaches~\cite{MUSE,papadakis2012using,Metallaxis}
%enhance the code coverage information by modifying a statement with
%mutation operators, and then collecting code coverages when executing
%the mutated programs with the test cases. They apply suspiciousness
%score formulas in the same manner as the spectrum-based FL approaches on
%the code coverage matrix for each original statement and its mutated
%ones. Despite the improvement, MBFL are not effective for the bugs
%that require the fixes that are more complex than a mutation
%(Section~\ref{motivexample}).

%{\em Machine learning (ML)} and {\em deep learning (DL)}
%have been used in fault localization. DeepFL~\cite{DeepFL}
%computes for each faulty method a vector with +200 scores in which
%each score is computed via a specific feature, e.g., a spectrum-based
%or mutation-based formula, or a code complexity metric. Despite its
%success, the accuracy of DeepFL is still limited. A reason could be
%that it uses various calculated scores from different formulas as a
%proxy to learn the suspiciousness of a faulty element, instead of
%fully exploiting the code coverage. Some formulas, such
%as the spectrum- and mutation-based formulas, inherently suffer from
%the issues as explained earlier with the statements covered by
%both failing and passing test cases.

Despite their successes, the state-of-the-art FL approaches still do
not support locating all the fixing locations that need to be repaired
at the same time in the same fix. In real-world software development,
there are several bugs that require a fix to multiple lines of code in
one or multiple hunks in the same or different methods. {\em The
  fixing changes to those lines of code are dependent to one another
  and need to be made in the same fix for the program to pass the test
  cases}. For those bugs, applying the fixing change to one statement
at a time will not make the program pass the test case after the
change to one statement.
%
This capability to detect the fixing locations of the co-changes in a
fix for a bug (let us call it {\em Co-change Fixing Locations} ({\em
  CC Fix Locations})) is important for both the manual process of bug
fixing as well as the automated process of program repairing. For
manual process, such capability will save effort and time for
developers in locating all the buggy statements that need to be fixed
at the same time. For automated program repair (APR), such capability
will enable an APR model to correctly and completely make the changes
to fix a bug.

From the ranked list of suspicious statements returned from an
existing FL model, a naive approach to detect CC Fix Locations would
be to take the top $k$ statements in that list and to consider them as
to be fixed together. This solution might not be effective
because the mechanisms used in the state-of-the-art FL approaches have
never considered the co-change nature of those fixes. Our empirical
evaluation also confirmed that (Section~\ref{eval:sec}).

Detecting all the fixing locations at multiple statements in
potentially multiple methods is challenging. A naive solution would be
detecting the potential methods that need to be fixed together and
then detecting potential statements that need to be changed together
in each of those methods. However, doing so will create a comfounding
effect from the inaccuracy of the detection of buggy methods to the
one of the buggy statements.

{\bf FIXME.} We propose {\tool}, a fault localization approach for
buggy statements/methods ...

The contributions of this paper are listed as follows:

{\bf 1. Novel code coverage representation.} Our representation
enables fully exploiting test coverage matrix and taking advantage of
the CNN model in image recognition to localize~faults.

{\bf 2. {\tool}: Novel DL-based fault localization approach.} Test
case ordering and three sources of information allow treating FL as a
pattern recognition. Without ordering and statement dependencies, the
CNN model will not work~well.

{\bf 3. Extensive empirical evaluation.} We evaluated our model
against the most recent FL models at the statement and method levels,
in both within-project and cross-project settings, and for both C and
Java. Our replication package is available
at~\cite{FaultLocalization2021}.

