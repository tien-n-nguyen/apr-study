\subsection{Empirical Methodology}

\subsubsection{Datasets}
We perform our valuation on three datasets that have been used
in the prior research in APR~\cite{icse20}:
Defects4J~\cite{defects4j}, Bugs.jar~\cite{saha2018bugs}, and
BigFix~\cite{yioopsla19}. The version of the Defects4J dataset we used
in this study is the V1.2.0~\cite{defects4j} with 395 bugs
from 6 Java projects. For each bug in a project, Defects4J has the
faulty and fixed versions of the project. There are relevant test
cases for each bug. With the \code{Diff} comparison between faulty and
fixed versions of a project, we can identify the buggy statements. The
Bugs.jar dataset contains 1,158 bugs and patches from 8 large, popular
open-source Java projects. The BigFix dataset contains +4.9 million
Java methods, and among them, +1.8 million Java methods are
buggy. There are also the corresponding bug fixes for the buggy
methods in each dataset as in Defects4J. We conducted all the
experiments on a server with 16 core CPU and a single Nvidia A100 GPU.

\subsubsection{Evaluation Metrics}

We use three evaluation metrics to evaluate the performance of \tool
and the baseline models.

{\bf 1. Correct Patches/Plausible Patches:} Correct patches are the
fixes that exactly match or have the same semantic meaning as the
fixes in the ground truth by real developers. Plausible patches are
the fixes that pass all the test cases, but might not match exactly
with the actual fixes by developers.

%It may contain the correct patches and the situation that
%the generated fixing is not correct but passes all test cases.
{\bf 2. P\%:} is the percentage of generated plausible patches to
be correct ones matching the ground truth by real developers.

{\bf 3. Top-$K$:} is the percentage of total bugs in which a correct
patch for a bug is in the ranked list of top-$K$ candidate patches.

\subsubsection{Evaluation Methodology.}
We use the following settings for different RQs:

{\bf RQ1. Comparison with the state-of-the-art DL-based APR Approaches on
  Defects4J Dataset.}

\underline{Baselines.} We compare {\tool} with the following
state-of-the-art deep learning (DL)-based APR baseline models:

%{\it Hercules \cite{hercules-icse19}: } is a novel APR technique that generalizes single-hunk repair to encompass a specific but significant class of multi-hunk repair problems.

%{\it Tbar \cite{tbar-issta19}: } is a template-based APR to build comprehensive% knowledge about the effectiveness of fix patterns.

{\bf SequenceR~\cite{chen2018sequencer}: } uses the machine
translation approach with sequence-to-sequence learning.

{\bf CoCoNuT~\cite{lutellier2020coconut}:} uses a context-aware neural
machine translation architecture to represent the buggy source code
and its surrounding context separately. However, it does not have the
dual learning for context learning and code transformation learning as
in {\tool}. It uses ensemble learning on the combination of
convolutional neural networks and a context-aware machine translation.

{\bf DLFix~\cite{icse20}: } is a two-tier DL-based model that
treats APR as code transformation learning from the prior bug fixes
and the surrounding code contexts. It follows a cascading architecture
between two models of context learning and transformation learning.

{\bf CURE~\cite{cure-icse21}: } is a machine-translation-based program repair
technique using GPT~\cite{radford2018improving} that by design parses, models, and searches source code, as
opposed to natural language text, to fix bugs automatically. It treats
context separately from the buggy statements as in CoCoNuT. It does
not have the dual learning for context learning and code
transformation learning as in {\tool}.


%Tien
%In this RQ, for %Hercules and
%Tbar, we directly use the results reported in their original paper because they are pattern-based approaches.

For all approaches under study in this RQ, we used the BigFix as the
training dataset and evaluated on Defects4J dataset. 
%For each bug in Defects4J, we used the remaining bugs in Defects4J as the developing dataset to fine-tune the model, and used the fine-tuned model to predict the fix for the bug. 
We used the correct fixing locations for the models to perform fixing
and used test cases for validation steps.  We set a 5-hour limit for
the validation step for all approaches as in previous
work~\cite{icse20,tbar-issta19}.


\underline{Parameter tuning.} We tuned the baselines with the
parameters that mentioned in their papers. We tuned \tool with the
following parameters: {\em epoch}, {\em batch size}, {\em learning
  rate}, {\em embedding length}, the {\em max number of children nodes
  $P$}, {\em the max children node depth} $Q$, and {\em the beam
  search size $n$}. Our model was tuned with the following key
hyper-parameters to obtain the best performance: (1) Epoch size (i.e.,
100, 200, 300); (2) Batch size (i.e., 64, 128, 256); (3) Learning rate
(i.e., 0.001, 0.003, 0.005, 0.010); (4) Vector length of word
representation and its output (i.e., 150, 200, 250, 300).

{\bf RQ2. Comparison with State-of-the-art DL-based APR Approaches on
  Large Datasets.}

\underline{Baselines.} We compare {\tool} with the following state-of-the-art DL-based APR baseline models:

{\bf SequenceR~\cite{chen2018sequencer}, CoCoNuT~\cite{lutellier2020coconut}, DLFix~\cite{icse20}, CURE~\cite{cure-icse21}:} as explained in RQ1.

{\bf CODIT~\cite{chakrabortycodit}:} is the DL-based APR approach using
sequence-to-sequence machine translation model with the abstractions on tree
structures to learn the code transformations for bug fixing.

{\bf Tufano'19~\cite{tufano2019learning}:} is also a DL-based approach
aiming to learn code changes by adopting neural machine translation
with code abstractions and filtering via program analysis.

We did not compare with CODIT~\cite{chakrabortycodit} and
Tufano'19~\cite{tufano2019learning} in RQ1 on Defects4J dataset,
because they do not have the validation step in their approaches to
run on Defects4J. It is unfair if we compared them with the other
approaches having the validation step.

We compared {\tool} with those two baselines in this experiment for
RQ2 running on two large datasets, in which we directly ran the fixing
step without the validation. The two large datasets do not contain the
corresponding test cases for the bugs. Thus, we ran a model
without the validation step on these two datasets.

%The two new baselines in this RQ, including Tufano 19\' and CODIT are not designed for the Defects4J dataset, and there is not validation step in these two approaches. Therefore, comparing these two approaches to other baselines and \tool with the validation step is not fair. So we did not compare these two approaches as baselines in RQ1.

%As for this RQ, for all baselines and \tool, we directly run the
%fixing step without validation. So it is fair to add these two
%baselines.

%We ran the baselines and \tool on two big datasets in this RQ, including Bugs.jar and BigFix.

In this RQ2, as running a model on each dataset, we randomly splitted
it into 80\%/10\%/10\% for training, developing, and testing. We used
the same slitting scheme for all models.


\underline{Parameter tuning.} We tuned the baselines with the
parameters mentioned in their papers and tuned {\tool} in the same
manner as in RQ1 with autoML \cite{NNI}.

{\bf RQ3. Overlapping Analysis.} To further study the comparative
result between a baseline model $M$ and {\tool}, we analyzed and
counted the number of bugs that were fixed by {\tool} and were missed
by $M$, the number of bugs that were fixed by $M$ and were missed
by {\tool}, and the number of bugs that were fixed by both.


{\bf RQ4. Impact Analysis of Dual Learning on Performance.}

\underline{Baselines.} To study the contributions of dual-learning in
{\tool}, we built two variants:

(1) \textbf{Transformation-only model:} In this variant, the
context learning model is removed from {\tool} and only the bug-fixing
code transformation learning model is kept. The result allows us to
understand the contributions of the context learning model (CCL).

%for training in step 2 of \tool.

(2) \textbf{Cascading model:} We also built another variant model in
which we removed the cross-stitch unit for dual learning, and we
connected the context learning model (CCL) to the transformation
learning model (CTL) in a cascading manner as in DLFix~\cite{icse20}
(the context learning result is added as an additional input of the
transformation learning model).

%the other naive model, the
%two-tier model, as the baseline. As for the two-tier model, we removed
%the dual-learning from {\tool} and make the statement-level program
%repair is dependent on the output of the method-level program repair.

In this RQ, we used the same process and parameter tuning as in the
experiments for the other RQs. We run all models on BigFix.

%the baselines and \tool in this RQ. Therefore, the parameters that
%eed to be tuned in both baselines are the same as the ones of \tool.

{\bf RQ5. Evaluation on C/C++ Projects.}  To evaluate {\tool} on C/C++
code, we ran it on the C/C++ benchmark
Codeflaws~\cite{tan2017codeflaws} with 3902 bugs. 
We used the same process and setting as in RQ2.
