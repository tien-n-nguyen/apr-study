\subsection{Empirical Methodology}

\subsubsection{Datasets}
We used three datasets that were used in the prior APR research:
Defects4J-v1.2~\cite{defects4j}, Bugs.jar~\cite{saha2018bugs}, and
BigFix~\cite{yioopsla19}. Defects4J-v1.2 has 395 bugs from 6 Java
projects. For each bug in a project, it has the faulty and fixed
versions of the project with test cases. Bugs.jar has 1,158 bugs and
patches from 8 large Java projects. BigFix contains +4.9 million Java
methods, and among them, +1.8 million Java methods are buggy. Bugs.jar
and BigFix contain bug fixes but no test case. We conducted all the
experiments on a server with 16 core CPU and a single Nvidia A100 GPU.

%We perform our valuation on three datasets that have been used
%in the prior research in APR~\cite{icse20}:
%Defects4J~\cite{defects4j}, Bugs.jar~\cite{saha2018bugs}, and
%BigFix~\cite{yioopsla19}. The version of the Defects4J dataset we used
%in this study is the V1.2.0~\cite{defects4j} with 395 bugs
%from 6 Java projects. For each bug in a project, Defects4J has the
%faulty and fixed versions of the project. There are relevant test
%cases for each bug. With the \code{Diff} comparison between faulty and
%fixed versions of a project, we can identify the buggy statements. The
%Bugs.jar dataset contains 1,158 bugs and patches from 8 large, popular
%open-source Java projects. The BigFix dataset contains +4.9 million
%Java methods, and among them, +1.8 million Java methods are
%buggy. There are also the corresponding bug fixes for the buggy
%methods in each dataset as in Defects4J. We conducted all the
%experiments on a server with 16 core CPU and a single Nvidia A100 GPU.

\subsubsection{Evaluation Metrics}

We use the following three metrics:

%to evaluate the performance of \tool and the baseline models.

{\bf 1. Correct Patches/Plausible Patches:} Correct patches are the
fixes that exactly match or have the same semantic meaning as the
developers' fixes in the oracle. For Defects4J (a small dataset), we
manually checked semantic equivalence, however, for Bugs.jar and
BigFix, we used the criteria of exact-matching with the actual
fixes in the oracle.
%
Plausible patches are the fixes that pass all the test cases, but
might not match exactly with the actual fixes.

%It may contain the correct patches and the situation that
%the generated fixing is not correct but passes all test cases.
{\bf 2. P\%:} is the percentage of the generated plausible patches
that exactly match with the actual fixes.
%to be correct ones matching the ground truth by real developers.

{\bf 3. Top-$K$:} is the percentage of the total bugs in which a correct
patch for a bug is in the ranked list of top-$K$ candidate patches.

\subsubsection{Evaluation Methodology.\\}
%We use the following settings for different RQs:

{\bf RQ1. Comparison with DL-based APR Approaches on Defects4J.}
%\underline{Baselines.}
We compare {\tool} with the following
DL-based baselines:

%{\it Hercules \cite{hercules-icse19}: } is a novel APR technique that generalizes single-hunk repair to encompass a specific but significant class of multi-hunk repair problems.

%{\it Tbar \cite{tbar-issta19}: } is a template-based APR to build comprehensive% knowledge about the effectiveness of fix patterns.

{\bf SequenceR~\cite{chen2018sequencer}: } uses the machine
translation approach with sequence-to-sequence learning.

{\bf CoCoNuT~\cite{lutellier2020coconut}:} uses a context-aware neural
machine translation architecture to represent the buggy code
and its context separately.
%However, it does not have the dual learning for context learning and
%code transformation learning as in {\tool}.
It uses ensemble learning on the combination of convolutional neural
networks and a context-aware machine translation.

{\bf DLFix~\cite{icse20}:} DLFix has key differences with {\tool}.
First, CCL and CTL are different. DLFix uses code summarization to
collapse the
%buggy and fixed
subtrees, and uses the new trees with summarized nodes to train
CCL. Second, {\em CCL cascadingly connects to CTL} in which the
vector for the summarized node is used as a {\em weight} representing the
impact from CCL to CTL. The weight vector is used in a cross-product
with all the vectors of the nodes in the subtrees to train CTL. In
{\tool}, dual-task learning is used between CCL and CTL.

%buggy and fixed subtrees to train the CTL model.

%is a two-tier DL-based model that treats APR as code transformation
%learning from the prior bug fixes and the surrounding code
%contexts. It follows a cascading architecture between two models of
%context learning and transformation learning.

{\bf CURE~\cite{cure-icse21}: } is a machine-translation-based program
repair technique using GPT~\cite{radford2018improving} that parses,
models, and searches source code, as opposed to natural language text,
to fix bugs automatically. It treats context separately from the buggy
statements.
%as in CoCoNuT.

%It does not have the dual learning for context learning and code
%transformation learning as in {\tool}.


%Tien
%In this RQ, for %Hercules and
%Tbar, we directly use the results reported in their original paper because they are pattern-based approaches.

For all approaches under study in this RQ, we used the BigFix as the
training dataset and evaluated on Defects4J dataset. 
%For each bug in Defects4J, we used the remaining bugs in Defects4J as the developing dataset to fine-tune the model, and used the fine-tuned model to predict the fix for the bug. 
We use two additional steps for all approaches in comparison:

(1) Fault localization (FL): Conceptually, any FL
tools can be used to produce an ordered list of suspicious
statements that require fixes. We chose Ochiai
algorithm~\cite{abreu2006evaluation, pearson2017evaluating}, which has
been widely used in
APR~\cite{jiang2018shaping,xiong2017precise,koyuncu2018fixminer,xin2017leveraging,wen2018context,liu2018lsrepair}.
After Ochiai localizes a buggy line, all of the AST nodes including
intermediate ones that are labeled by the parser with that buggy line
are collected into a buggy subtree.

(2) Patch validation: Once {\tool} generates a ranked list of
candidate patches, we use a validation
technique~\cite{saha2017elixir,jiang2018shaping} to validate each
candidate.
%Once a candidate patch pases all available test cases, {\tool} stops
%and reports the candidate patch for manual investigation. We report
%the patches that are exactly matched or semantically equivalent to
%the ground-truth fixes in Defects4J.
We set a 5-hour limit for the validation step for all approaches as in
previous work~\cite{icse20,tbar-issta19}.
%Tien
For CURE~\cite{cure-icse21}, we also ran it on its original setting
with unlimited time, with beam search (beam size of 1,000). It
generates 1,000 candidates and the top 5,000 of them are validated.

%I add the CURE* which is the same setting as their original paper. The
%setting is that using beam search, the beam size is 1000, generates
%10000 candidates, validate on top 5000.)

We also used the correct fixing locations for the models to perform
fixing in our study (i.e., without fault localization).
%and used test cases for validation steps.



\underline{Parameter tuning.} We tuned the baselines with the
parameters that mentioned in their papers. We tuned \tool with autoML
\cite{NNI} for the following parameters: {\em epoch}, {\em batch
  size}, {\em learning rate}, {\em embedding length}, the {\em max
  number of children nodes $P$}, and {\em the max children node depth}
$Q$. Our model was tuned with the following key hyper-parameters to
obtain the best performance: (1) Epoch size (i.e., 100, 200, 300); (2)
Batch size (i.e., 64, 128, 256); (3) Learning rate (i.e., 0.001,
0.003, 0.005, 0.010); (4) Vector length of word representation and its
output (i.e., 150, 200, 250, 300); (5) max number of children nodes
$P$ (i.e. 4, 5, 6); (6) max children node depth $Q$ (i.e. 3, 4, 5).

{\bf RQ2. Comparison with DL-based APR Approaches on
  Large Datasets.} We compare {\tool} with the following baselines:
%\underline{Baselines.} We compare {\tool} with the following
%state-of-the-art DL-based APR baseline models:
{\bf Sequen\-ceR~\cite{chen2018sequencer},
  CoCoNuT~\cite{lutellier2020coconut}, DLFix~\cite{icse20},
  CURE~\cite{cure-icse21}:} as in RQ1.

{\bf CODIT~\cite{chakrabortycodit}:} is a DL-based APR approach
using sequence-to-sequence machine translation model with the
abstractions on tree structures to learn the code transformations for
bug fixing.

{\bf Tufano'19~\cite{tufano2019learning}:} is a DL-based approach
aiming to learn code changes by adopting neural machine translation
with code abstractions and filtering via program analysis.

We did not compare with CODIT~\cite{chakrabortycodit} and
Tufano'19~\cite{tufano2019learning} in RQ1, because they do not have
the validation step in their approaches to run on Defects4J. It is
unfair if we compare them with the other approaches having the
validation step. However, we compared {\tool} with them in RQ2
since the two datasets Bugs.jar and BigFix do not have the
test cases for the bugs. Thus, we ran the fixing step of each
model without patch validation on those two datasets.

%We compared {\tool} with those two baselines in this experiment for
%RQ2 running on two large datasets, in which we directly ran the fixing
%step without the validation. The two large datasets do not contain the
%corresponding test cases for the bugs. Thus, we ran a model
%without the validation step on these two datasets.

%The two new baselines in this RQ, including Tufano 19\' and CODIT are not designed for the Defects4J dataset, and there is not validation step in these two approaches. Therefore, comparing these two approaches to other baselines and \tool with the validation step is not fair. So we did not compare these two approaches as baselines in RQ1.

%As for this RQ, for all baselines and \tool, we directly run the
%fixing step without validation. So it is fair to add these two
%baselines.

%We ran the baselines and \tool on two big datasets in this RQ, including Bugs.jar and BigFix.

In RQ2, as running each model, we randomly splitted a dataset
into 80\%/10\%/10\% for training, tuning (validation), and testing.

%We used the same splitting scheme for all models.

In RQ2, we did not use a fault localization tool because Bugs.jar and
BigFix do not have test cases. We used correct fixing locations.

\underline{Parameter tuning:} the same as in RQ1.

%We tuned the baselines with the
%parameters mentioned in their papers and tuned {\tool} as in RQ1.

%in the same manner as in RQ1 with autoML \cite{NNI}.

{\bf RQ3. Overlapping Analysis.} For a baseline model $M$ and {\tool},
we analyzed the number of bugs that were fixed by {\tool}
and missed by $M$, the number of bugs that were fixed by $M$ and
missed by {\tool}, and the number of bugs that were fixed by
both.


{\bf RQ4. Impact Analysis of Dual Learning on Performance.}
%underline{Baselines.} To study the contributions of dual-learning in
%{\tool},
We compare {\tool} with two of its variants:

(1) \textbf{Transformation-only model:} In this variant,
CCL was removed from {\tool} and only CTL was kept. The result
allows us to access the contribution of context learning.
%the context learning model is removed from {\tool} and only the
%bug-fixing code transformation learning model is kept. The result
%allows us to understand the contributions of the context learning
%model (CCL).

%for training in step 2 of \tool.

(2) \textbf{Cascading model:} We also built another variant model in
which we removed the cross-stitch unit for dual-task learning. CCL is
connected to CTL in a cascading manner.
%in which the output of CCL corresponding to a buggy subtree is
%directly used as the input of CTL.
Note: this cascading model differs from DLFix~\cite{icse20} (explained
in RQ1). First, CCL and CTL are different from those in DLFix, which
uses a code summarization technique. Second, in the cascading model,
the output of CCL corresponding to a buggy subtree is directly used as
the input of CTL. In DLFix~\cite{icse20}, the summarized vector is
used in a cross-product to represent the impact from CCL to CTL.

%. In Cascading_Model (Figure_5), part of the CCL output corresponding
%to the buggy subtree is used directly as input to CTL.
%We connected the context learning model (CCL) to the transformation
%learning model (CTL) in a cascading manner as in DLFix~\cite{icse20}
%(the context learning result is added as an additional input of the
%transformation learning model).

%the other naive model, the
%two-tier model, as the baseline. As for the two-tier model, we removed
%the dual-learning from {\tool} and make the statement-level program
%repair is dependent on the output of the method-level program repair.

In this RQ, we used the same process and parameter tuning as in the
experiments for the other RQs. We run all models on BigFix.

%the baselines and \tool in this RQ. Therefore, the parameters that
%eed to be tuned in both baselines are the same as the ones of \tool.

{\bf RQ5. Evaluation on C/C++ Projects.}  To evaluate {\tool} on C/C++
code, we ran it on the C/C++ benchmark
Codeflaws~\cite{tan2017codeflaws} with 3,902 bugs. 
We used the same process and setting as in RQ2.
